{"cmd": "import json\nimport torch\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom model import *; from utils import *; from config import *\nfrom tqdm.auto import tqdm\n\ndef run(rank, config:Config):\n    # init\n    print(f\"Parallel on rank {rank}.\")\n    pbar1 = tqdm(range(epochs*steps), desc='Progress', total=epochs*steps, leave = True, position=0, colour='blue')\n\n    adj_matr, obj_coll, hml_coll = initdata(hml_len = 4)\n\n    paras = para(config.hp, CommonModule)\n    ModelT = (Encoder(**paras, node_dim = adj_matr.shape[0], encoder_layers = config.hp['enc_layers'], device = rank),\n              PathModule(**paras, node_dim = adj_matr.shape[0], clipp = config.hp['clipp'], device = rank))\n\n    model = TPPModel(*ModelT)\n    baseline_model = TPPModel(*ModelT)\n    baseline_model.load_state_dict(model.state_dict(), strict = False)\n\n    optimiser = optim.Adam(model.parameters(), lr)\n\n    cost_graph = []\n    \n    adj_matr_batch = adj_matr.unsqueeze(0).to(torch.float32) \n    obj_coll = obj_coll.to(torch.float32)\n\n    for _ in range(epochs):\n        cost_t = 0\n        for _ in range(steps):\n            # hml = randomly sample\n            hml = hml_coll.to(torch.float32)[:config.hp['batch_dim']]\n            \n            route, loss, log_p = model(adj_matr_batch, obj_coll, hml)\n            _, baseline, _ = model(adj_matr, obj_coll, hml, baseline = True)\n            \n            optimiser.zero_grad()\n            loss.backward()\n            optimiser.step()\n            \n            pbar1.update()\n\n        if ospttest(cost, baseline) < 0.05:\n            baseline_model.load_state_dict(model.state_dict())\n        \n        cost_graph.append(cost_t)\n\n    torch.save(model.state_dict(), 'model.pth')\n    cleanup()\n\n    return cost_graph\n\nepochs = 20 #100\nsteps = 1000 #2500; decent performance for nodes ~ 5 at 100\nlr = 1e-4\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \nif __name__ == '__main__':\n    use_distributed_training = False\n\n    hyperparas = {\n            'market_dim': 20,\n            'product_dim': 20,\n            'unif_dim': 128,\n            'batch_dim': 1, #fix batch processing if possible\n            'mlp_dim' : 512,\n            'vec_dim': 16,\n            'enc_layers': 3,\n            'head_dim': 8,\n            'clipp' : 10\n    }\n    with open('hyperparas.json', 'w') as f:\n        json.dump(hyperparas, f, indent=4)\n\n    config = Config(use_distributed_training, hyperparas)\n    #loss_graph = mp.spawn(run, args=(config,), nprocs=config.world_size, join=True)\n    run('cpu', config)", "cmd_opts": " --cell_id=NONE -s", "import_complete": 1, "terminal": "nvimterm"}