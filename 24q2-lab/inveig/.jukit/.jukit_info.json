{"cmd": "\nimport torch, torch.nn as nn\nimport numpy as np, matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\ndef seq_mlp(init, mlp, fin, act):\n    modules = [nn.Linear(init, mlp[0]), act]\n    for i in range(len(mlp) - 1):\n        modules.append(nn.Linear(mlp[i], mlp[i+1]))\n        modules.append(act)\n\n    modules.append(nn.Linear(mlp[-1], fin)) #self.spl for spline\n\n    return modules\n\n# |%%--%%| <oLmbyD3rxN|Nmb5q8f54j>\n\nclass EvalEig(nn.Module):\n    def __init__(self, para):\n        super().__init__()\n        self.rn = para['rn']\n        self.rm = para['rm']\n    \n    def fixed_tr(self, para_ptl, ptl_form):\n        coeffs_tr = torch.ones(self.batch_dim, 1)\n        r_dsc_d = self.r_dsc.view(1,-1)\n        if ptl_form == \"coulomb\":\n            #coeffs_tr = 1 + torch.rand(self.batch_dim, 1)/10\n            return -coeffs_tr*para_ptl/r_dsc_d\n        elif ptl_form == \"yukawa\":\n            return coeffs_tr*para_ptl*(-1/r_dsc_d + torch.exp(-r_dsc_d/40)*r_dsc_d/120)\n\n    def set_evl(self, evl):\n        self.evl = evl\n        \n        ptl_modules = seq_mlp(init = 1, mlp = para['mlp'], fin = 1, act = nn.ReLU())\n        self.ptl_mlp = nn.Sequential(*ptl_modules)\n        rad_modules = seq_mlp(init = 1, mlp = para['mlp'], fin = evl.shape[0], act = nn.Tanh())\n        self.rad_mlp = nn.Sequential(*rad_modules)        \n\n    def grad_nn(self, x, y):\n        grad_list = []\n        for i in range(y.shape[0]):\n            grad = torch.autograd.grad(\n                outputs = y[i], inputs = x, grad_outputs = torch.ones_like(y[i]), \n                retain_graph = True, create_graph = True)[0]\n            grad_list.append(grad)\n        grad = torch.stack(grad_list)\n        return grad\n\n    def spectral_err(self, rad, ptl, evl):\n        r_dsc = torch.linspace(self.rm/self.rn, self.rm, self.rn, requires_grad=True)\n        self.r_dsc = r_dsc.detach()\n        r_rs = r_dsc.unsqueeze(0).expand(evl.shape[0],-1)\n\n        evl_rs = evl.view(-1,1)\n        ptl_rs = ptl(r_dsc.view(-1,1)).view(1,-1)\n        self.ptl_rs = ptl_rs.detach()\n        rad_rs = rad(r_dsc.view(-1,1)).transpose(0,1)\n        #print(\"u(r) : \", rad_rs.shape, rad_rs)\n\n        rad_d = self.grad_nn(r_dsc, rad_rs)\n        #print(\"u'(r) : \", rad_d.shape, rad_d)\n\n        rad_dd = self.grad_nn(r_dsc, rad_d)\n        #print(\"u''(r) : \", rad_dd.shape, rad_dd)\n        \n        #plt.figure()\n        #plt.plot(r_dsc.detach(), rad_rs[0].detach())\n        #plt.plot(r_dsc.detach(), rad_d[0].detach())\n        #plt.plot(r_dsc.detach(), rad_dd[0].detach())\n        #plt.show()\n\n        # rad output : evl_N x self.rn\n        error_mtr = -rad_dd + ptl_rs * rad_rs - evl_rs * rad_rs\n\n        return error_mtr\n\n    def forward(self):\n        error_mtr = self.spectral_err(self.rad_mlp, self.ptl_mlp, self.evl)\n        error = torch.sqrt(torch.sum(error_mtr**2))\n\n        return error\n\n# |%%--%%| <Nmb5q8f54j|xr3NyHiOFF>\n\npara = {\n        'rm' : 1000,\n        'rn' : 10000,\n\n        # model\n        'mlp' : [100,100],\n\n        # training\n        'epoch' : 1000,\n        'lr' : 1e-2,\n\n        # loss regularisation\n        'reg1' : 1e-2,\n        'reg2' : 1e-2,\n}\n\nmodel = EvalEig(para)\n\n#|%%--%%| <xr3NyHiOFF|WRUrqeNtUf>\n\nevl_tr = -torch.arange(1,10).to(torch.float32)**(-2)\nmodel.set_evl(evl_tr)\n\n# |%%--%%| <WRUrqeNtUf|dyJzB2ZAcV>\n\n#model.load_state_dict(torch.load('1.pth'))\n\noptimiser = torch.optim.Adam(model.parameters(), lr = para['lr'])\nepochs = para['epoch']\npbar = tqdm(range(epochs), desc='Progress', total=epochs, leave = True, position=0, colour='blue')\nloss_list = [[],[]]\n\nfor e in range(epochs):\n    #with torch.autograd.detect_anomaly():\n    loss = model()\n    #print(loss.item())\n\n    optimiser.zero_grad()\n    loss.backward()\n    optimiser.step()\n\n    #pbar.update()\n    print(loss.item())\n", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "nvimterm"}